{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Домашняя работа часть 2  (5 баллов): совмещение спарсификации и квантизации.\n",
    "\n",
    "\n",
    "В этом задании вам предстоит совместить спарсификацию и квантизацию.\n",
    "\n",
    "В это заданаии мы предлагаем вам самим реализовать функции квантизации и спарсисикации и далее совместить их вместе. В качестве базовых методов предлагаем взять GPTQ и SparseGPT, использование других методов не исключается.\n",
    "\n",
    "Функция квантизации также должна возвращать уровень сжатия модели, считаем что понижение битовости в два раза соотвествует сжатию в два раза,  зануление половины весов так же соответствует сжатию в два раза.\n",
    "\n",
    "Что бы упросить задание, мы не будем квантизовать всю модель, а квантизуем только по слоям.\n",
    "\n",
    "Резальтат работы это метрики L1 и L2  для $|C(W)X^T - WX^T|$ для каждого слоя, $С$  - функция одновременной спарсификации и квантизации.\n",
    "\n",
    "\n",
    "В папке llama7b_weights содержатся веса для одоного слоя LLaMа7b и сопутствующие активации в папке llama7b_act_scales (аггрегировые по датасету)\n",
    "\n",
    "\n",
    "В качестве результата нужно будет сдать ноутбук с решением и метриками.\n",
    "\n",
    "### Баллы\n",
    "\n",
    "Релизация алгортима спарсификации - 1 балл\n",
    "Релизация алгортима квантизации - 1 балл\n",
    "Объеденение двух алгоритмов - 3 балла"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4096, 11008])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load weights\n",
    "\n",
    "FOLDER = \"../llama7b_weights/\"\n",
    "names = os.listdir(FOLDER)\n",
    "weight_paths =  {name.replace(\".pt\",'') :os.path.join(FOLDER, name) for name in names}\n",
    "names = [name.replace(\".pt\",'') for name in names]\n",
    "W = torch.load(weight_paths[names[1]])\n",
    "W.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([11008, 11008])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Расчет матрицы гессе для слоя\n",
    "X = torch.load(\"../llama7b_act_scales/Llama-2-7b-hf.pt\")[names[1]]\n",
    "\n",
    "H = torch.outer(X, X)\n",
    "\n",
    "damp = 0.01 * torch.mean(torch.diag(H))\n",
    "diag = torch.arange(X.shape[0])\n",
    "H[diag, diag] += damp\n",
    "\n",
    "H = torch.linalg.cholesky(H)\n",
    "H = torch.cholesky_inverse(H)\n",
    "H = torch.linalg.cholesky(H, upper=True)\n",
    "Hinv = H\n",
    "H.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для вдохновения рекомендуем воспользоваться кодов QUIK\n",
    "\n",
    "https://github.com/IST-DASLab/QUIK/blob/9558d7121c698174fc93940e0b52c38c746c97ea/experiments/quik_utils.py#L80"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ваше решение тут ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Пример расчета ошибки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model.layers.20.self_attn.k_proj': (tensor(11.6558), tensor(2.6132), 0),\n",
       " 'model.layers.20.mlp.down_proj': (tensor(158.4233), tensor(10.0173), 0),\n",
       " 'model.layers.20.mlp.gate_proj': (tensor(4.2363), tensor(1.6319), 0),\n",
       " 'model.layers.20.self_attn.o_proj': (tensor(4.2832), tensor(1.6623), 0),\n",
       " 'model.layers.20.self_attn.v_proj': (tensor(4.8678), tensor(1.7560), 0),\n",
       " 'model.layers.20.mlp.up_proj': (tensor(3.5155), tensor(1.4972), 0),\n",
       " 'model.layers.20.self_attn.q_proj': (tensor(10.3696), tensor(2.4367), 0)}"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  Фкнкцию ниже можно переписать для своего удобства \n",
    "\n",
    "def test(your_quantization_function):\n",
    "    c = your_quantization_function\n",
    "    result = dict()\n",
    "    for name in names:\n",
    "        X = torch.load(\"../llama7b_act_scales/Llama-2-7b-hf.pt\")[name]\n",
    "        W = torch.load(weight_paths[name])\n",
    "        X = X.float()\n",
    "        W = W.float()\n",
    "        WQ, compress_size = c(W)\n",
    "        l2 = ((W@X.T - WQ@X.T)**2).mean()\n",
    "        l1 = (W@X.T - WQ@X.T).abs().mean()\n",
    "        result[name] = (l2,l1, compress_size)\n",
    "    return  result\n",
    "\n",
    "\n",
    "def dummy_compress(X):\n",
    "    X = torch.round(X)\n",
    "    compress_size = 0\n",
    "    return X, compress_size\n",
    "\n",
    "\n",
    "test(dummy_compress)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
