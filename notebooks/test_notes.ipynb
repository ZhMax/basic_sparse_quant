{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import datasets\n",
    "from datasets import load_dataset\n",
    "\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    LlamaTokenizer,\n",
    "    LlamaTokenizerFast,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    TrainingArguments\n",
    ")\n",
    "\n",
    "from transformers import GPTQConfig, BitsAndBytesConfig\n",
    "\n",
    "from peft import (\n",
    "    prepare_model_for_kbit_training,\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    PeftModel,\n",
    "    TaskType\n",
    ")\n",
    "\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update({'font.size': 16})\n",
    "\n",
    "IGNORE_INDEX = -100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import OPTForCausalLM\n",
    "model = \"/home/sparse_quant_methods/weights/opt350m_gptq_w4_a16\"\n",
    "model = OPTForCausalLM.from_pretrained(model, torch_dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0669, -0.1167, -0.0835,  ...,  0.0334,  0.0000, -0.1001],\n",
       "        [ 0.0669,  0.1167,  0.0835,  ..., -0.0334,  0.0334,  0.0835],\n",
       "        [ 0.0500,  0.1338,  0.0835,  ..., -0.0334,  0.0167,  0.0835],\n",
       "        ...,\n",
       "        [-0.0669,  0.0669,  0.0334,  ..., -0.0669,  0.0167,  0.0835],\n",
       "        [ 0.0500,  0.0669,  0.0167,  ...,  0.0669, -0.0500, -0.0669],\n",
       "        [-0.0669,  0.0669, -0.0167,  ..., -0.0669,  0.0167,  0.0669]],\n",
       "       dtype=torch.bfloat16, requires_grad=True)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.decoder.layers[0].self_attn.k_proj.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0659, -0.1113, -0.0879,  ...,  0.0640, -0.0153, -0.0859],\n",
       "        [ 0.0586,  0.1182,  0.0894,  ..., -0.0498,  0.0315,  0.0791],\n",
       "        [ 0.0466,  0.1250,  0.0845,  ..., -0.0337,  0.0084,  0.0884],\n",
       "        ...,\n",
       "        [-0.0640,  0.0640,  0.0334,  ..., -0.0635,  0.0344,  0.0625],\n",
       "        [ 0.0562,  0.0654,  0.0208,  ...,  0.0623, -0.0625, -0.0625],\n",
       "        [-0.0625,  0.0620, -0.0104,  ..., -0.0630,  0.0337,  0.0625]],\n",
       "       dtype=torch.bfloat16, requires_grad=True)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.decoder.layers[0].self_attn.k_proj.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"/home/LLaMA/huggingface/Llama-2-7b-hf\"\n",
    "\n",
    "# Load pretrained tokenizer\n",
    "tokenizer_kwargs = {\n",
    "    \"use_fast\": True,\n",
    "    \"revision\": 'main',\n",
    "    \"trust_remote_code\": True,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python  /home/spars_quant/sparsegpt/opt.py \\\n",
    "    --model /home/LLaMA/huggingface/opt-350m \\\n",
    "    --dataset wikitext2 \\\n",
    "    --sparsity 0.5 \\\n",
    "    --wbits 4 \\\n",
    "    --save /home/sparse_quant_methods/weights/opt350m_sparsegpt_w4_a16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python /home/Quantization/smoothquant/examples/generate_act_scales.py \\\n",
    "    --model-name /home/LLaMA/huggingface/tulu-2-7b \\\n",
    "    --output-path /home/LLaMA/huggingface/act_scales/tulut-2-7b-hf.pt \\\n",
    "    --dataset-path /home/LLM_compression/outliers_identification/datasets/val.jsonl.zst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python /home/sparse_quant_methods/quik/experiments/fake_quant/opt.py \\\n",
    "    --model /home/LLaMA/huggingface/opt-350m \\\n",
    "    --path_to_act_scales /home/sparse_quant_methods/quik/experiments/act_scales/opt_350m.pt \\\n",
    "    --path_to_save_quant_model /home/sparse_quant_methods/weights/opt350m_w4_a16 \\\n",
    "    --fp_features 128 \\\n",
    "    --a_bits 16 \\\n",
    "    --w_bits 4 \\\n",
    "    --w_clip \\\n",
    "    --dataset wikitext2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python /home/sparse_quant_methods/wanda/main_opt.py \\\n",
    "    --model /home/LLaMA/huggingface/opt-350m \\\n",
    "    --prune_method wanda \\\n",
    "    --sparsity_ratio 0.5 \\\n",
    "    --sparsity_type unstructured \\\n",
    "    --save_model /home/sparse_quant_methods/weights/opt350m_wanda_50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid decimal literal (2429943922.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[3], line 2\u001b[0;36m\u001b[0m\n\u001b[0;31m    --model /home/LLaMA/huggingface/opt-350m \\\u001b[0m\n\u001b[0m                                          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid decimal literal\n"
     ]
    }
   ],
   "source": [
    "python /home/sparse_quant_methods/wanda/main_opt.py \\\n",
    "    --model /home/LLaMA/huggingface/opt-350m \\\n",
    "    --prune_method sparsegpt \\\n",
    "    --sparsity_ratio 0.5 \\\n",
    "    --sparsity_type unstructured \\\n",
    "    --save_model /home/sparse_quant_methods/weights/opt350m_sparsegpt_50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_eval --model hf \\\n",
    "    --model_args \"pretrained=/home/sparse_quant_methods/weights/opt350m_gptq_w4_a16\" \\\n",
    "    --tasks winogrande \\\n",
    "    --batch_size 4 \\\n",
    "    --num_fewshot 0 \\\n",
    "    --device cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    (python /home/LLM_Compression/QUIK/experiments/fake_quant/llama.py --model /home/llm_compression/LLaMA/Llama-2-13b-hf --path_to_act_scales /home/LLM_Compression/QUIK/experiments/act_scales/Llama-2-13b-hf.pt --path_to_save_quant_model /home/llm_compression/Quantization/Quik/weights_llama13b/llama13b_3w_16a_quant_params --fp_features 128 --a_bits 16 --w_bits 3 --w_clip --dataset wikitext2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python /home/sparse_quant_methods/wanda/main.py --model /home/LLaMA/huggingface/Llama-2-7b-hf --prune_method wanda --sparsity_ratio 0.5 --sparsity_type unstructured --save ./wanda/llama7b_sparsity_50 --save_model ./wanda/llama7b_sparsity_50"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
